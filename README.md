# HADOOP_CLUSTER
Building hadoop cluster from scratch (without third party distributions like clouidera etc) with tools like hive, sqoop etc for processing 20 petabytes of data at a time.

There are totally six files. 
Order of building system is in C series (i.e, c1, c2, c3 etc as written at the end of the file names).
With the help of jdbc, communication with hive is done unless if you like to run hive from terminal.
Commands file helps to import data and process data.
This project is done for cisco and they were able to succesfully run for 20 petabytes of data.
